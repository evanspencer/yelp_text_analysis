{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QmE_uHUrVTZC"
      },
      "outputs": [],
      "source": [
        "!pip install json\n",
        "!pip install sklearn\n",
        "!pip install sklearn.feature\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install pyldavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#imports go here, not all ended up being used etc, these are ones we used, some were tried and didnt work how expected or werent needed etc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from spacy.lang.en.examples import sentences\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "import string\n",
        "import gensim\n",
        "import os\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "import nltk; nltk.download('stopwords')\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_formats = ['retina']\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import fbeta_score\n",
        "import time"
      ],
      "metadata": {
        "id": "hes5ytQNVTZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in datasets"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Y-Rm4PdUVTZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#read in review file.\n",
        "data_file = open(\"yelp_academic_dataset_review.json\", encoding=\"utf8\")\n",
        "data = []\n",
        "for line in data_file:\n",
        "    data.append(json.loads(line))\n",
        "review_df = pd.DataFrame(data)\n",
        "data_file.close()\n",
        "#read in business file.\n",
        "data_file = open(\"yelp_academic_dataset_business.json\", encoding=\"utf8\")\n",
        "data = []\n",
        "for line in data_file:\n",
        "    data.append(json.loads(line))\n",
        "business_df = pd.DataFrame(data)\n",
        "data_file.close()\n",
        "\n",
        "## another way to read in json file for review\n",
        "with open('yelp_academic_dataset_business.json') as json_file:\n",
        "    data = json_file.readlines()\n",
        "    # this line below may take at least 8-10 minutes of processing for 4-5 million rows. It converts all strings in list to actual json objects.\n",
        "    data = list(map(json.loads, data))\n"
      ],
      "metadata": {
        "id": "L-jBib67VTZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analyis"
      ],
      "metadata": {
        "id": "LU6r0AvGflxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "id": "clbn_EX6fris"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "business_df.head()\n",
        "business_df.isnull().sum()"
      ],
      "metadata": {
        "id": "2Up9mTWffvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "f = sns.heatmap(business_df.isnull(),yticklabels=False, cbar=False, cmap = 'viridis')  # plotting the null values across all fields"
      ],
      "metadata": {
        "id": "tMj7DB_BgDMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.countplot(x='is_open',data=business_df); #plotting the number of closed and open businesses"
      ],
      "metadata": {
        "id": "QAmj4jZqgIXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='stars',data=business_df); # plotting the frequency of reviews across starts"
      ],
      "metadata": {
        "id": "Sf_hsS88gM2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(business_df['review_count'].apply(np.log1p)); #plotting the review count"
      ],
      "metadata": {
        "id": "QowBvHPEgREb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "by_state = business_df.groupby('state')   #grouping the data by state\n",
        "by_state"
      ],
      "metadata": {
        "id": "jD881_sCgVMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install squarify "
      ],
      "metadata": {
        "id": "0LmUHrwBgZvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import squarify    # pip install squarify (algorithm for treemap)\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "a = by_state['business_id'].count()   # identify the number of business ids by state\n",
        "\n",
        "a.sort_values(ascending=False,inplace=True)\n",
        "\n",
        "squarify.plot(sizes= a[0:15].values, label= a[0:15].index, alpha=0.9)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "FAYIMh0Sgdkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the number of businesses across different categories\n",
        "cats=business_df['categories']  \n",
        "cats\n",
        "cats_ser = cats.value_counts()\n",
        "cats_ser\n",
        "\n",
        "\n",
        "cats_df = pd.DataFrame(cats_ser)\n",
        "cats_df.reset_index(inplace=True)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "f = sns.barplot( y= 'index',x = 'categories' , data = cats_df.iloc[0:20])\n",
        "f.set_ylabel('Category')\n",
        "f.set_xlabel('Number of businesses')"
      ],
      "metadata": {
        "id": "qBrpvG1yg687"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting word cloud for business names\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                          width=1200,\n",
        "                      stopwords = STOPWORDS,\n",
        "                          height=1000\n",
        "                         ).generate(str(business_df['name']))\n",
        "\n",
        "\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "lk68XzXHhFES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data cleaning\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0hc5VSPuVTZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "review_df = review_df.drop(['review_id','useful','funny','cool'], axis = 1)\n",
        "# clean business file\n",
        "business_df = pd.read_json(business_json, lines=True)\n",
        "\n",
        "business_df_CA = df_business[df_business[\"state\"]==\"CA\"]\n",
        "\n",
        "\n",
        "#converting city to lower case\n",
        "business_df_CA[\"city\"] = business_df_CA.city.str.lower().apply(lambda x: x)\n",
        "#removing spaces from the city\n",
        "business_df_CA[\"city\"] = business_df_CA.city.str.strip().apply(lambda x: x)\n",
        "# removing extra characters from \"santa barbara\"\n",
        "business_df_CA[\"city\"] = business_df_CA.city.replace(to_replace=\"santa barbra\", value=\"santa barbara\")\n",
        "business_df_CA[\"city\"] = business_df_CA.city.replace(to_replace=\"santa barbara ap\", value=\"santa barbara\")\n",
        "business_df_CA[\"city\"] = business_df_CA.city.replace(to_replace=\"santa barbara,\", value=\"santa barbara\")\n",
        "business_df_CA[\"city\"] = business_df_CA.city.replace(to_replace=\"santa  barbara\", value=\"santa barbara\")\n",
        "business_df_CA[\"city\"] = business_df_CA.city.replace(to_replace=\"santa barbara & ventura counties\", value=\"santa barbara\")\n",
        "\n",
        "#couting the occurances of cities\n",
        "business_df_CA.groupby(\"city\").city.count().sort_values(ascending = False)\n",
        "#filtering out top 10 cities of california\n",
        "business_df_CA = business_df_CA[(df_business_CA[\"city\"] == \"santa barbara\") | (business_df_CA[\"city\"] == \"goleta\")|\n",
        "               (business_df_CA[\"city\"] == \"carpinteria\")| (business_df_CA[\"city\"] == \"isla vista\")|\n",
        "               (business_df_CA[\"city\"] == \"montecito\") | (business_df_CA[\"city\"] == \"summerland\")\n",
        "\n",
        "               ]\n",
        "#writing the cleaned business file into .csv\n",
        "business_df_CA.to_csv(\"C:/Users/raval/Desktop/Yelp Dataset/Business_Filtered.csv\")\n",
        "#reading business file into dataframe\n",
        "business_df = pd.read_csv(business_df_CA)"
      ],
      "metadata": {
        "id": "Zk3O586aVTZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#merge business and review df\n",
        "yelp_reviews = pd.merge(business_df, review_df, on=\"business_id\", how=\"left\")\n",
        "# filtering out records with categories containing \"restaurant\" or \"food\"\n",
        "yelp_reviews[\"categories\"] = yelp.categories.str.lower().apply(lambda x: x)\n",
        "yelp_reviews[\"boolean_findings_food\"] = yelp.categories.str.contains(\"restaurant\") | yelp.categories.str.contains(\"food\")\n",
        "\n",
        "#filter out food data\n",
        "yelp_food = yelp[yelp[\"boolean_findings_food\"]==True]\n",
        "#filter out non food data\n",
        "yelp_not_food = yelp[yelp[\"boolean_findings_food\"]==False]\n",
        "#store respective data into .csv files\n",
        "yelp_not_food.to_csv(\"C:/Users/raval/Desktop/Yelp Dataset/yelp_not_food.csv\")\n",
        "yelp_food.to_csv(\"C:/Users/raval/Desktop/Yelp Dataset/yelp_food.csv\")"
      ],
      "metadata": {
        "id": "BEiSDsu5VTZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#these columns are dropped from the restaurant/food dataset\n",
        "yelp_reviews = yelp_reviews.drop(['Unnamed: 0','Unnamed: 0.1','latitude','longitude',\n",
        "                                  'hours', 'user_id', 'boolean_findings_food', 'address', 'postal_code',\n",
        "                                  'business_id', 'state'], axis = 1)\n",
        "#rename the columns for stars_x(business review rating) and stars y(actual review rating)\n",
        "yelp_reviews.columns = ['business_rating' if x=='stars_x' else 'rating'\n",
        "                        if x=='stars_y' else x for x in yelp_reviews.columns]\n"
      ],
      "metadata": {
        "id": "gDeYaVczVTZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#look at data types of our dataset\n",
        "#we see that date is an object which may affect EDA later\n",
        "# we also want to drop the time as it is unnecessary from any analysis\n",
        "print(yelp_reviews.dtypes)"
      ],
      "metadata": {
        "id": "LDbBayqKVTZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#convert date to type datetime and drop the time\n",
        "yelp_reviews['date'] =  pd.to_datetime(yelp_reviews['date'], infer_datetime_format=True)\n",
        "yelp_reviews['date'] = yelp_reviews['date'].apply(lambda x: x.date())"
      ],
      "metadata": {
        "id": "XLO9OF0JVTZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews.columns = ['business_rating' if x=='stars_x'\n",
        "                        else 'rating' if x=='stars_y' else x for x in yelp_reviews.columns]"
      ],
      "metadata": {
        "id": "0GFB8c-gVTZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews = yelp_reviews.drop(['Unnamed: 0','Unnamed: 0.1','latitude','longitude', 'hours', 'user_id', 'boolean_findings_food', 'address', 'postal_code', 'business_id', 'state'], axis = 1)"
      ],
      "metadata": {
        "id": "qqcrophbVTZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## here is the final df that is used for everything\n",
        "##### can use this import here to bring it back in whenever"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Z1iuJe7_VTZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews = pd.read_csv('/content/drive/Shareddrives/programming project/yelp_reviews_6A_v4.csv')"
      ],
      "metadata": {
        "id": "zzFMF4nHVTZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text cleaning"
      ],
      "metadata": {
        "collapsed": false,
        "id": "PY7lUdOWVTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#stop words used for text processing\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stop_words = en.Defaults.stop_words\n",
        "stop_words|= {'come','try','go','get','make','drink','restaurant','place',\n",
        "                  'would','really','like','great','came','got', 'santa', 'barbara', 'order', 'plate', 'dish', 've', 'd', 's', 'll', 'm', 're'}\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "qG7IsdBLVTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#pre process the text analysis\n",
        "def strip_newline(series):\n",
        "    return [review.replace('\\n','') for review in series]\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def bigrams(words, bi_min=15):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "\n",
        "def lemmatization(texts):\n",
        "     texts_out = []\n",
        "     for sent in texts:\n",
        "         doc = nlp(\" \".join(sent))\n",
        "         texts_out.append([token.lemma_ for token in doc])\n",
        "     return texts_out\n"
      ],
      "metadata": {
        "id": "UQc3Ic2vVTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews['new_text'] = strip_newline(yelp_reviews.text)\n",
        "words = list(sent_to_words(yelp_reviews.new_text))\n",
        "words = remove_stopwords(words)\n",
        "bigram = bigrams(words)\n",
        "bigram = [bigram[review] for review in words]\n",
        "bigram = lemmatization(bigram)\n",
        "yelp_reviews['new_text'] = bigram"
      ],
      "metadata": {
        "id": "V7SFSpj7VTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews['new_text'] = yelp_reviews['new_text'].apply(', '.join)"
      ],
      "metadata": {
        "id": "MA0a3bmwVTZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sentiment analysis using vader"
      ],
      "metadata": {
        "collapsed": false,
        "id": "azTQ1jNrVTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#sentiment analysis add column giving polarization rating\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "yelp_reviews['scores'] = yelp_reviews['new_text'].apply(lambda new_text: sid.polarity_scores(new_text))"
      ],
      "metadata": {
        "id": "M8U8VwjOVTZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#get a compound score based off our sentiment scores of each word\n",
        "yelp_reviews['polarity_score']  = yelp_reviews['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "yelp_reviews.head()"
      ],
      "metadata": {
        "id": "I5j6dUuVVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews.head()\n",
        "#create column classifying as positive or negative\n",
        "yelp_reviews['polarity'] = yelp_reviews['polarity_score'].apply(lambda c: 2 if c >= .3 else 0 if c <= -.3 else 1)\n",
        "#convert to int\n",
        "yelp_reviews['rating'] = yelp_reviews['rating'].astype(int)"
      ],
      "metadata": {
        "id": "Y0L7fbtQVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "##use this if you want to only show final polarity score only\n",
        "#yelp_reviews = yelp_reviews[['name', 'rating', 'new_text', 'polarity_score']]\n"
      ],
      "metadata": {
        "id": "feXD7LsQVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#positive wordcloud\n",
        " from PIL import Image\n",
        " from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "\n",
        "pos_review = yelp_reviews[yelp_reviews[\"polarity\"]==\"positive\"]\n",
        "txt = \" \".join(review.lower() for review in pos_review[\"new_text\"])\n",
        "mask = np.array(Image.open(\"thumbsup.png\"))\n",
        "wordcloud = WordCloud(mask = mask, background_color='white', colormap='summer').generate(txt)\n",
        "plt.imshow(wordcloud,interpolation = \"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('positive_wc.png', bbox_inches='tight')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "GSdVoUDEVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#negative word cloud\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "neg_review = yelp_reviews[yelp_reviews[\"polarity\"] == \"negative\"]\n",
        "txt = \" \".join(review.lower() for review in neg_review[\"new_text\"])\n",
        "mask = np.array(Image.open(\"thumbsdown.png\"))\n",
        "wordcloud = WordCloud(mask=mask, background_color='white', colormap='autumn').generate(txt)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('negative_wc.png', bbox_inches='tight')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "UFFtzPFKVTZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## here is tfidf models"
      ],
      "metadata": {
        "collapsed": false,
        "id": "IP2rSbePVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kMLUOXLFVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "yelp_reviews['rating'] = yelp_reviews[\"rating\"]-1\n",
        "#ratings for labels werent matching up so had to do a rating scale of 0-4"
      ],
      "metadata": {
        "id": "8gL7bcgsVTZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### below commented out cell can be used for binning and rerunning the tfid"
      ],
      "metadata": {
        "collapsed": false,
        "id": "_bK5FCtgVTZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# yelp_reviews['rating'] = yelp_reviews['rating'].apply(lambda i: 2 if i >= 3 else 0 if i <= 1  else 1)"
      ],
      "metadata": {
        "id": "qB66RUM7VTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# First, create a vectorizer object. we only consider 4000 words\n",
        "x = yelp_reviews[\"new_text\"].values.astype(str)\n",
        "y = yelp_reviews[\"rating\"].values\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "x = vectorizer.fit_transform(x).toarray()\n",
        "\n",
        "process_time = round(time.time()-start,2)\n",
        "\n",
        "print(\"Vectorizing using TF-IDF approach took {} seconds\".format(process_time))\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "QhsICitXVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "## sometimes this was needed to label encode target variable not always tho\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "wbrBPY6lVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "GI5H0KRRVTZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xgboost"
      ],
      "metadata": {
        "collapsed": false,
        "id": "35VSo0tZVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "start = time.time()\n",
        "\n",
        "evalset = [(x_train, y_train), (x_test,y_test)]\n",
        "classifier = XGBClassifier(n_estimators=100, learning_rate=.01, eval_metric='mlogloss', num_class = 1)\n",
        "\n",
        "classifier.fit(x_train,y_train, eval_set=evalset)\n",
        "\n",
        "preds = classifier.predict(x_test)\n",
        "\n",
        "process_time = round(time.time()-start,2)"
      ],
      "metadata": {
        "id": "ahTMdOTCVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"XGB classification took {} seconds\".format(process_time))\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(\"Accuracy of our XGB Classifier is: %.2f%%\" % (accuracy * 100.0))\n",
        "#show log loss learning curve\n",
        "from matplotlib import pyplot\n",
        "# plot learning curves\n",
        "pyplot.plot(results['validation_0']['mlogloss'], label='train')\n",
        "pyplot.plot(results['validation_1']['mlogloss'], label='test')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "pyplot.savefig('logloss_1_5')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "XP_-cM6hVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "##plot our confusion matrix\n",
        "\n",
        "pyplot.show()\n",
        "plt.subplots(figsize=(6, 6))\n",
        "sns.heatmap(confusion_matrix(y_pred=preds, y_true=y_test), annot=True, fmt=\".1f\", linewidths=1.5, cmap=\"autumn\")\n",
        "plt.savefig('boost_cm.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_L8pAFl7VTZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## naive bayes"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3ApdnbZdVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# First, create a vectorizer object. we only consider 4000 words\n",
        "x = yelp_reviews[\"new_text\"].values.astype(str)\n",
        "y = yelp_reviews[\"rating\"].values\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "x = vectorizer.fit_transform(x).toarray()\n",
        "\n",
        "process_time = round(time.time()-start,2)\n",
        "\n",
        "print(\"Vectorizing using TF-IDF approach took {} seconds\".format(process_time))\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "lqIg7MqfVTZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "cKC3GyNFVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_train, y_train)\n",
        "nb_preds = clf.predict(x_test)\n",
        "nb_acc = accuracy_score(y_test, nb_preds)\n",
        "print(\"Accuracy of our nb Classifier is: %.2f%%\" % (nb_acc * 100.0))"
      ],
      "metadata": {
        "id": "zrlIo5VfVTZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## logistic regression"
      ],
      "metadata": {
        "collapsed": false,
        "id": "mWQGBkqjVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# First, create a vectorizer object. we only consider 4000 words\n",
        "x = yelp_reviews[\"new_text\"].values.astype(str)\n",
        "y = yelp_reviews[\"rating\"].values\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "x = vectorizer.fit_transform(x).toarray()\n",
        "\n",
        "process_time = round(time.time()-start,2)\n",
        "\n",
        "print(\"Vectorizing using TF-IDF approach took {} seconds\".format(process_time))\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "i_-UmnROVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "9kR8ltyEVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "clf = LogisticRegression(max_iter=10000).fit(x_train, y_train)\n",
        "log_preds = clf.predict(x_test)\n",
        "log_acc = accuracy_score(y_test, log_preds)"
      ],
      "metadata": {
        "id": "dbOfjBiNVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Accuracy of our log Classifier is: %.2f%%\" % (log_acc * 100.0))"
      ],
      "metadata": {
        "id": "3iOuWvToVTZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## topic modeling"
      ],
      "metadata": {
        "collapsed": false,
        "id": "pctuanDMVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
      ],
      "metadata": {
        "id": "K4JXD3QUVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#def for text processing again if needed\n",
        "def strip_newline(series):\n",
        "    return [review.replace('\\n','') for review in series]\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "\n",
        "def lemmatization(texts):\n",
        "     texts_out = []\n",
        "     for sent in texts:\n",
        "         doc = nlp(\" \".join(sent))\n",
        "         texts_out.append([token.lemma_ for token in doc])\n",
        "     return texts_out"
      ],
      "metadata": {
        "id": "anu3JolyVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def get_corpus(df):\n",
        "#this gets all of our pieces needed for lda\n",
        "\n",
        "    df['new_text'] = strip_newline(df.new_text)\n",
        "    words = list(sent_to_words(df.new_text))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram = bigrams(words)\n",
        "    bigram = [bigram[review] for review in words]\n",
        "    lemma = lemmatization(bigram)\n",
        "    id2word = gensim.corpora.Dictionary(lemma)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in lemma]\n",
        "    return corpus, id2word, bigram, lemma"
      ],
      "metadata": {
        "id": "u6aQc8HCVTZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#use get_corp function to get our corpus for lda\n",
        "corpus, id2word, bigram, lemma = get_corpus(yelp_reviews)"
      ],
      "metadata": {
        "id": "QJ3B8TSvVTZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#this is lda model, use multicore to use more cpu processing power\n",
        "lda_model = gensim.models.ldamulticore.LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    num_topics=20,\n",
        "    id2word=id2word,\n",
        "    chunksize=100,\n",
        "    workers=11, # Change to Num. Processing Cores - 1\n",
        "    passes=50,\n",
        "    eval_every = 1,\n",
        "    per_word_topics=True)\n",
        "lda_model.save('lda_model.model')"
      ],
      "metadata": {
        "id": "D1GBbTdsVTZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#print topics\n",
        "lda_model.print_topics(20,num_words=20)[:20]"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "FEze0JvjVTZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#create interactive graphic and save it for use on github\n",
        "pyLDAvis.enable_notebook()\n",
        "viz = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "\n",
        "pyLDAvis.save_html(viz, 'lda_V2.html')"
      ],
      "metadata": {
        "id": "2j9KYoe2VTZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#map the reviews data to the model, v is the reviews transformed to word vectors by doc2bow\n",
        "yelp_map = lda_model[corpus[0:len(corpus)]]\n",
        "\n",
        "#assign topic to each review\n",
        "import operator\n",
        "topic = []\n",
        "for i in yelp_map:\n",
        "    #find the topic with the highest proportions\n",
        "    t = max(i, key = operator.itemgetter(1))\n",
        "    topic.append(t[0])"
      ],
      "metadata": {
        "id": "dhjUvQYBVTZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#change topics to actual words/categories\n",
        "topic_name = {0:'Atmosphere', 1:'Food', 2:'Atmosphere',\n",
        "             3:'Food', 4:'Service', 5:'Food',\n",
        "             6:'Price', 7:'Service', 8:'Food', 9:'Atmosphere', 10:'Food', 11:'Service', 12:'Food', 13:'Food', 14:'Atmosphere', 15:'Food', 16:'Food', 17:'Atmosphere', 18:'Service', 19:'Price'}\n",
        "\n",
        "yelp_reviews = yelp_reviews.replace({'topic':topic_name})"
      ],
      "metadata": {
        "id": "FLHiv4GGVTZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment score again, this is here because when we originally did this we did\n",
        "#not have it added to that dataset \n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# Vader sentiment analysis\n",
        "\n",
        "def sentiment_analyzer_scores(review, nth):\n",
        "    score_list = []\n",
        "    for i in range(nth):\n",
        "        score = analyzer.polarity_scores(review[i])\n",
        "        for cat, point in score.items():\n",
        "            #only take compound score\n",
        "            if cat == 'compound':\n",
        "                score_list.append(point)\n",
        "    return score_list"
      ],
      "metadata": {
        "id": "8zg1wleLWmxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create polarity score \n",
        "def topic_scores(dataset):\n",
        "\n",
        "    reviews = dataset['new_text']\n",
        "    reviews_score = sentiment_analyzer_scores(reviews, len(reviews))\n",
        "    reviews_score = pd.DataFrame(reviews_score)\n",
        "    yelp_reviews['topic_score'] = reviews_score\n",
        "    business_scores = yelp_reviews.loc[:, ['name', 'topic', 'topic_score']]\n",
        "\n",
        "    #normalize the score to 0-5\n",
        "    business_scores = round(abs((business_scores.groupby(['name', 'topic']).sum() /\n",
        "                   business_scores.groupby(['name', 'topic']).count()) * 5), 1)\n",
        "\n",
        "    pd.set_option('display.max_rows', 100)\n",
        "    return business_scores\n",
        "\n",
        "categorized_score= topic_scores(yelp_reviews)\n",
        "categorized_score"
      ],
      "metadata": {
        "id": "MWmP6HGyXoWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply polarity score from our dataset with this version here\n",
        "# def topic_scores(dataset):\n",
        "\n",
        "#     reviews = dataset['new_text']\n",
        "#     business_scores = yelp_reviews.loc[:, ['name', 'topic', 'polarity_score']]\n",
        "\n",
        "#     #normalize the score to 0-5\n",
        "#     business_scores = round(abs((business_scores.groupby(['name', 'topic']).sum() /\n",
        "#                    business_scores.groupby(['name', 'topic']).count()) * 5), 1)\n",
        "\n",
        "#     pd.set_option('display.max_rows', 100)\n",
        "#     return business_scores\n",
        "\n",
        "# categorized_score= topic_scores(yelp_reviews)\n",
        "# categorized_score"
      ],
      "metadata": {
        "id": "1KAVsfYGX21j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}